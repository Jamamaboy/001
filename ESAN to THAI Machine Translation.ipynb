{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamamaboy/001/blob/main/ESAN%20to%20THAI%20Machine%20Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WV-XR5kjvZfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "หมายเหตุ ที่เป็น error เพราะเนื่องจาก reset runtime และ ตอนที่ทำการ\\\n",
        "Installation Unsloth จะต้อง Restart session ก่อนที่จะSeting Unsloth\\\n",
        "ผมไม่อาจrunต่อได้เนื่องจากมีเวลาไม่พอ"
      ],
      "metadata": {
        "id": "w-DK7EBRvp-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ลิงค์สไลด์**\n",
        "[ลิงค์สไลด์](https://www.canva.com/design/DAGghMf-Bk0/PKGDNyRZ-CJ0BphoPdgV3Q/edit?utm_content=DAGghMf-Bk0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)\n",
        "canva\n",
        "\n"
      ],
      "metadata": {
        "id": "hrBxRkI4x3_8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91npXb0YKN9"
      },
      "source": [
        "### Installation unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zJVp5PZMYKN_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks and Kaggle notebooks!\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHifSfy1YKOB"
      },
      "source": [
        "### Seting Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "e69b3c8b-b325-4de8-f4ac-817babd93c99"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1dca45c36c67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;31m# Choose any! We auto support RoPE Scaling internally!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "72f03697-b6c1-48d8-ca25-46f7b84e773d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FastLanguageModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-50d4941685c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = FastLanguageModel.get_peft_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n\u001b[1;32m      5\u001b[0m                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import Data"
      ],
      "metadata": {
        "id": "CUXCTj2mW978"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "L9jUfOVbXCGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Get the Kaggle credentials from Colab's userdata\n",
        "KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n",
        "KAGGLE_KEY = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Ensure credentials exist\n",
        "if not KAGGLE_USERNAME or not KAGGLE_KEY:\n",
        "    raise ValueError(\"Error: Kaggle credentials not found. Set them in Colab's userdata or manually.\")\n",
        "\n",
        "# Create the kaggle.json content\n",
        "kaggle_credentials = {\n",
        "    \"username\": KAGGLE_USERNAME,\n",
        "    \"key\": KAGGLE_KEY\n",
        "}\n",
        "\n",
        "# Write the credentials to kaggle.json\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "    json.dump(kaggle_credentials, f)\n",
        "\n",
        "# Set the correct permissions\n",
        "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "# Try downloading the dataset again\n",
        "!kaggle competitions download -c superai5-esan-to-thai-machine-translation\n"
      ],
      "metadata": {
        "id": "t-0H9nfHXEAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"superai5-esan-to-thai-machine-translation.zip\" -d dataset"
      ],
      "metadata": {
        "id": "Pun1o__yXFvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('dataset/train.csv')\n",
        "df_val = pd.read_csv('dataset/val.csv')\n",
        "df_test = pd.read_csv('dataset/test.csv')\n",
        "df_sub_sample = pd.read_csv('dataset/sample_submission.csv')\n",
        "df_submission = pd.read_csv('dataset/submission.csv')"
      ],
      "metadata": {
        "id": "tZhrcVOZW__u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "y0IdoOSuXS7K",
        "outputId": "ba1c3b34-e903-4533-e0a9-a4f55219e355"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f45937856810>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: ช่วยแปลภาษาจากภาษาถิ่นอีสานเป็นภาษาไทยกลาง\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"input\"]  # คอลัมน์ที่มีข้อความภาษาอีสาน\n",
        "    outputs = examples[\"output\"]  # คอลัมน์ที่มีคำแปลภาษากลาง\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # เติม input และ output ลงในแม่แบบ\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# สมมติว่า df_combined เป็น DataFrame ที่มีคอลัมน์ \"input\"และ\"output\"\n",
        "from datasets import Dataset\n",
        "\n",
        "# แปลง DataFrame เป็น Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df_combined)\n",
        "\n",
        "# จัดรูปแบบข้อมูลสำหรับการ fine-tune\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "8xPemG2Rt6UJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "## Prediction\n",
        "หลังจากที่เราเทรนโมเดลเสร็จ ก็จะทำการPredictionในฟังชั่น <**translate_isaan**>\\\n",
        "จากนั้นจะทำการเก็บ sentence ที่ได้จาก Prediction มาเก็บใน <**translations**> แล้วทำการจัดเก็บใน dataframe ของ <**df_combined_sub**> (combined_submission)เพื่อที่จะปรับให้อยู่ใน format พร้อมส่ง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def translate_isaan(isaan_text):\n",
        "    inference_prompt = alpaca_prompt.format(\n",
        "        isaan_text,\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "    result = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # ดึงเฉพาะส่วนคำตอบ\n",
        "    try:\n",
        "        translated = result.split(\"### Response:\")[1].strip()\n",
        "    except:\n",
        "        translated = result  # กรณีไม่พบรูปแบบที่คาดหวัง\n",
        "\n",
        "    return translated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ตรวจสอบว่ามีแถวไหนที่มีค่า output เป็น NaN\n",
        "rows_to_translate = df_combined_sub['output'].isna()\n",
        "\n",
        "# สร้างลิสต์เพื่อเก็บผลลัพธ์การแปล\n",
        "translations = []\n",
        "\n",
        "# ทำนายคำแปลสำหรับแต่ละแถวที่มีค่า output เป็น NaN\n",
        "for idx, row in df_combined_sub[rows_to_translate].iterrows():\n",
        "    isaan_text = row['input']\n",
        "    translated_text = translate_isaan(isaan_text)\n",
        "    translations.append({\n",
        "        'id': row['id'],\n",
        "        'input': isaan_text,\n",
        "        'predicted_output': translated_text\n",
        "    })\n",
        "\n",
        "# สร้าง DataFrame ใหม่จากผลลัพธ์\n",
        "translation_results = pd.DataFrame(translations)\n",
        "\n",
        "# พิมพ์ผลลัพธ์\n",
        "print(translation_results)\n",
        "\n",
        "# หากคุณต้องการอัปเดต df_combined_sub ด้วยคำแปล:\n",
        "for idx, row in translation_results.iterrows():\n",
        "    df_combined_sub.loc[df_combined_sub['id'] == row['id'], 'output'] = row['predicted_output']"
      ],
      "metadata": {
        "id": "OqojkBZIdT7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined_sub"
      ],
      "metadata": {
        "id": "imUNrUiOfXco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_submission_fi = df_combined_sub\n",
        "df_submission_fi['output'] = df_submission_fi['output'].str.replace('\\n<|end_of_text|>', '', regex=False)\n"
      ],
      "metadata": {
        "id": "YyGFYxAgfY_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_fi"
      ],
      "metadata": {
        "id": "oWyrHteafkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install deepcut attacut"
      ],
      "metadata": {
        "id": "ikLHAnkpf9IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "df_submission_fi['word_tokenized_output'] = df_submission_fi['output'].apply(lambda x: word_tokenize(x, engine=\"newmm\"))\n",
        "df_submission_fi\n"
      ],
      "metadata": {
        "id": "0JQ1zHJLgFkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final = df_submission_fi.copy()\n",
        "df_submission_final.drop(columns=\"output\", inplace=True)\n",
        "df_submission_final.rename(columns={\"word_tokenized_output\": \"output\"}, inplace=True)\n",
        "df_submission_final\n"
      ],
      "metadata": {
        "id": "hzQdoRI7gpcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final.at[1, 'output'] = ['ไม่', 'รู้', 'ว่า', 'ใช้', 'แอป', 'นี้', 'ทำ', 'อะไร']\n"
      ],
      "metadata": {
        "id": "0e4FrWX9g6Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final.at[9, 'output'] = ['วันนี้', 'ทำ', 'อะไร','ไหม']"
      ],
      "metadata": {
        "id": "wzbNOYKDuYeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final.value_counts(\"input\")\n",
        "df_submission_final.shape\n",
        "df_submission_final.drop(columns='input',inplace=True)"
      ],
      "metadata": {
        "id": "WP4CNzQoh6Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final"
      ],
      "metadata": {
        "id": "Y4hkjWvTg_KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_final['output'] = df_submission_final['output'].apply(lambda x: ' '.join(x))\n",
        "df_submission_final"
      ],
      "metadata": {
        "id": "PmxWJ58rkNQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}